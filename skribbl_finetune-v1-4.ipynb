{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipSgY-L3z8kG"
   },
   "source": [
    "# Fine Tune Stable Diffusion\n",
    "\n",
    "Fine tuning Stable Diffusion on Pokemon, \n",
    "for more details see the [Lambda Labs examples repo](https://github.com/LambdaLabsML/examples). \n",
    "\n",
    "We recommend using a multi-GPU machine, for example an instance from [Lambda GPU Cloud](https://lambdalabs.com/service/gpu-cloud). If running on Colab this notebook is likely to need a GPU with >16GB of VRAM and a runtime with high RAM, which will almost certainly need Colab Pro or Pro+. (If you get errors suchs as `Killed` or `CUDA out of memory` then one of these is not sufficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bjNGOU6Pz8kH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stable-diffusion'...\n",
      "remote: Enumerating objects: 1747, done.\u001b[K\n",
      "remote: Total 1747 (delta 0), reused 0 (delta 0), pack-reused 1747\u001b[K\n",
      "Receiving objects: 100% (1747/1747), 73.93 MiB | 45.60 MiB/s, done.\n",
      "Resolving deltas: 100% (1079/1079), done.\n",
      "/home/ubuntu/stable-diffusion\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/ubuntu/.local/lib/python3.8/site-packages (22.3)\n",
      "Collecting pip\n",
      "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3\n",
      "    Uninstalling pip-22.3:\n",
      "      Successfully uninstalled pip-22.3\n",
      "Successfully installed pip-23.3.1\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
      "Obtaining taming-transformers from git+https://github.com/CompVis/taming-transformers.git@master#egg=taming-transformers (from -r requirements.txt (line 23))\n",
      "  Cloning https://github.com/CompVis/taming-transformers.git (to revision master) to ./src/taming-transformers\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/CompVis/taming-transformers.git /home/ubuntu/stable-diffusion/src/taming-transformers\n",
      "  Resolved https://github.com/CompVis/taming-transformers.git to commit 3ba01b241669f5ade541ce990f7650a3b8f65318\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hObtaining clip from git+https://github.com/openai/CLIP.git@main#egg=clip (from -r requirements.txt (line 24))\n",
      "  Cloning https://github.com/openai/CLIP.git (to revision main) to ./src/clip\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /home/ubuntu/stable-diffusion/src/clip\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hObtaining nomi from git+https://github.com/justinpinkney/nomi.git@e9ded23b7e2269cc64d39683e1bf3c0319f552ab#egg=nomi (from -r requirements.txt (line 25))\n",
      "  Cloning https://github.com/justinpinkney/nomi.git (to revision e9ded23b7e2269cc64d39683e1bf3c0319f552ab) to ./src/nomi\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/justinpinkney/nomi.git /home/ubuntu/stable-diffusion/src/nomi\n",
      "  Running command git rev-parse -q --verify 'sha^e9ded23b7e2269cc64d39683e1bf3c0319f552ab'\n",
      "  Running command git fetch -q https://github.com/justinpinkney/nomi.git e9ded23b7e2269cc64d39683e1bf3c0319f552ab\n",
      "  Resolved https://github.com/justinpinkney/nomi.git to commit e9ded23b7e2269cc64d39683e1bf3c0319f552ab\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hObtaining file:///home/ubuntu/stable-diffusion (from -r requirements.txt (line 26))\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch==1.12.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 2)) (1.12.1)\n",
      "Requirement already satisfied: torchvision==0.13.1 in /usr/lib/python3/dist-packages (from -r requirements.txt (line 3)) (0.13.1)\n",
      "Collecting albumentations==0.4.3 (from -r requirements.txt (line 4))\n",
      "  Downloading albumentations-0.4.3.tar.gz (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting opencv-python==4.5.5.64 (from -r requirements.txt (line 5))\n",
      "  Downloading opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.5/60.5 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pudb==2019.2 (from -r requirements.txt (line 6))\n",
      "  Downloading pudb-2019.2.tar.gz (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting imageio==2.9.0 (from -r requirements.txt (line 7))\n",
      "  Downloading imageio-2.9.0-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting imageio-ffmpeg==0.4.2 (from -r requirements.txt (line 8))\n",
      "  Downloading imageio_ffmpeg-0.4.2-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pytorch-lightning==1.4.2 (from -r requirements.txt (line 9))\n",
      "  Downloading pytorch_lightning-1.4.2-py3-none-any.whl (916 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m916.6/916.6 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting omegaconf==2.1.1 (from -r requirements.txt (line 10))\n",
      "  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting test-tube>=0.7.5 (from -r requirements.txt (line 11))\n",
      "  Downloading test_tube-0.7.5.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting streamlit>=0.73.1 (from -r requirements.txt (line 12))\n",
      "  Downloading streamlit-1.28.2-py2.py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting einops==0.3.0 (from -r requirements.txt (line 13))\n",
      "  Downloading einops-0.3.0-py2.py3-none-any.whl (25 kB)\n",
      "Collecting torch-fidelity==0.3.0 (from -r requirements.txt (line 14))\n",
      "  Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
      "Collecting transformers==4.22.2 (from -r requirements.txt (line 15))\n",
      "  Downloading transformers-4.22.2-py3-none-any.whl (4.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kornia==0.6 (from -r requirements.txt (line 16))\n",
      "  Downloading kornia-0.6.0-py2.py3-none-any.whl (367 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting webdataset==0.2.5 (from -r requirements.txt (line 17))\n",
      "  Downloading webdataset-0.2.5-py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchmetrics==0.6.0 (from -r requirements.txt (line 18))\n",
      "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fire==0.4.0 (from -r requirements.txt (line 19))\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gradio==3.1.4 (from -r requirements.txt (line 20))\n",
      "  Downloading gradio-3.1.4-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting diffusers==0.3.0 (from -r requirements.txt (line 21))\n",
      "  Downloading diffusers-0.3.0-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets==2.4.0 (from datasets[vision]==2.4.0->-r requirements.txt (line 22))\n",
      "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from albumentations==0.4.3->-r requirements.txt (line 4)) (5.3.1)\n",
      "Collecting imgaug<0.2.7,>=0.2.5 (from albumentations==0.4.3->-r requirements.txt (line 4))\n",
      "  Downloading imgaug-0.2.6.tar.gz (631 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.4/631.4 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from albumentations==0.4.3->-r requirements.txt (line 4)) (1.23.4)\n",
      "Collecting opencv-python-headless>=4.1.1 (from albumentations==0.4.3->-r requirements.txt (line 4))\n",
      "  Downloading opencv_python_headless-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: scipy in /home/ubuntu/.local/lib/python3.8/site-packages (from albumentations==0.4.3->-r requirements.txt (line 4)) (1.9.3)\n",
      "Requirement already satisfied: pygments>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from pudb==2019.2->-r requirements.txt (line 6)) (2.13.0)\n",
      "Collecting urwid>=1.1.1 (from pudb==2019.2->-r requirements.txt (line 6))\n",
      "  Downloading urwid-2.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from imageio==2.9.0->-r requirements.txt (line 7)) (7.0.0)\n",
      "Requirement already satisfied: future>=0.17.1 in /usr/lib/python3/dist-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 9)) (0.18.2)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 9)) (4.64.1)\n",
      "Collecting fsspec!=2021.06.0,>=2021.05.0 (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.4.2->-r requirements.txt (line 9))\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /usr/lib/python3/dist-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 9)) (2.9.1)\n",
      "Collecting pyDeprecate==0.3.1 (from pytorch-lightning==1.4.2->-r requirements.txt (line 9))\n",
      "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 9)) (21.3)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/.local/lib/python3.8/site-packages (from pytorch-lightning==1.4.2->-r requirements.txt (line 9)) (4.4.0)\n",
      "Collecting antlr4-python3-runtime==4.8 (from omegaconf==2.1.1->-r requirements.txt (line 10))\n",
      "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers==4.22.2->-r requirements.txt (line 15)) (3.0.12)\n",
      "Collecting huggingface-hub<1.0,>=0.9.0 (from transformers==4.22.2->-r requirements.txt (line 15))\n",
      "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.22.2->-r requirements.txt (line 15))\n",
      "  Downloading regex-2023.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers==4.22.2->-r requirements.txt (line 15)) (2.28.1)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers==4.22.2->-r requirements.txt (line 15))\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting braceexpand (from webdataset==0.2.5->-r requirements.txt (line 17))\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from fire==0.4.0->-r requirements.txt (line 19)) (1.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/lib/python3/dist-packages (from fire==0.4.0->-r requirements.txt (line 19)) (1.1.0)\n",
      "Collecting analytics-python (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading analytics_python-1.4.post1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting aiohttp (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading aiohttp-3.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting h11<0.13,>=0.11 (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastapi (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading fastapi-0.104.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting markdown-it-py[linkify,plugins] (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: matplotlib in /home/ubuntu/.local/lib/python3.8/site-packages (from gradio==3.1.4->-r requirements.txt (line 20)) (3.5.3)\n",
      "Collecting orjson (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading orjson-3.9.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from gradio==3.1.4->-r requirements.txt (line 20)) (1.5.1)\n",
      "Collecting paramiko (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading paramiko-3.3.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting pycryptodome (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting python-multipart (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydub (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting uvicorn (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: Jinja2 in /home/ubuntu/.local/lib/python3.8/site-packages (from gradio==3.1.4->-r requirements.txt (line 20)) (3.1.2)\n",
      "Collecting httpx (from gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading httpx-0.25.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pydantic in /home/ubuntu/.local/lib/python3.8/site-packages (from gradio==3.1.4->-r requirements.txt (line 20)) (1.10.2)\n",
      "Requirement already satisfied: importlib-metadata in /home/ubuntu/.local/lib/python3.8/site-packages (from diffusers==0.3.0->-r requirements.txt (line 21)) (5.0.0)\n",
      "Collecting pyarrow>=6.0.0 (from datasets==2.4.0->datasets[vision]==2.4.0->-r requirements.txt (line 22))\n",
      "  Downloading pyarrow-14.0.1-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.3.6 (from datasets==2.4.0->datasets[vision]==2.4.0->-r requirements.txt (line 22))\n",
      "  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets==2.4.0->datasets[vision]==2.4.0->-r requirements.txt (line 22))\n",
      "  Downloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets==2.4.0->datasets[vision]==2.4.0->-r requirements.txt (line 22))\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl.metadata (7.1 kB)\n",
      "Collecting responses<0.19 (from datasets==2.4.0->datasets[vision]==2.4.0->-r requirements.txt (line 22))\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting altair<6,>=4.0 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading altair-5.1.2-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 12)) (1.4)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /usr/lib/python3/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 12)) (4.0.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /usr/lib/python3/dist-packages (from streamlit>=0.73.1->-r requirements.txt (line 12)) (7.0)\n",
      "Collecting pillow (from imageio==2.9.0->-r requirements.txt (line 7))\n",
      "  Downloading Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting protobuf<5,>=3.20 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit>=0.73.1->-r requirements.txt (line 12)) (2.8.2)\n",
      "Collecting rich<14,>=10.14.0 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tenacity<9,>=8.1.0 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tzlocal<6,>=1.1 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting validators<1,>=0.2 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading validators-0.22.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading GitPython-3.1.40-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from streamlit>=0.73.1->-r requirements.txt (line 12)) (6.2)\n",
      "Collecting watchdog>=2.1.5 (from streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ftfy (from clip->-r requirements.txt (line 24))\n",
      "  Downloading ftfy-6.1.3-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/lib/python3/dist-packages (from altair<6,>=4.0->streamlit>=0.73.1->-r requirements.txt (line 12)) (3.2.0)\n",
      "Requirement already satisfied: toolz in /usr/lib/python3/dist-packages (from altair<6,>=4.0->streamlit>=0.73.1->-r requirements.txt (line 12)) (0.9.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->gradio==3.1.4->-r requirements.txt (line 20)) (19.3.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading yarl-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (28 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/lib/python3/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.3->-r requirements.txt (line 4)) (0.16.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata->diffusers==0.3.0->-r requirements.txt (line 21)) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=17.0->pytorch-lightning==1.4.2->-r requirements.txt (line 9)) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->gradio==3.1.4->-r requirements.txt (line 20)) (2022.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from Jinja2->gradio==3.1.4->-r requirements.txt (line 20)) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers==4.22.2->-r requirements.txt (line 15)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 15)) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 15)) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.22.2->-r requirements.txt (line 15)) (2019.11.28)\n",
      "Collecting urllib3<1.27,>=1.21.1 (from requests->transformers==4.22.2->-r requirements.txt (line 15))\n",
      "  Downloading urllib3-1.26.18-py2.py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting backports.zoneinfo (from tzlocal<6,>=1.1->streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading backports.zoneinfo-0.2.1-cp38-cp38-manylinux1_x86_64.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.0/74.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting monotonic>=1.5 (from analytics-python->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff==1.10.0 (from analytics-python->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading backoff-1.10.0-py2.py3-none-any.whl (31 kB)\n",
      "Collecting anyio<4.0.0,>=3.7.1 (from fastapi->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting typing-extensions (from pytorch-lightning==1.4.2->-r requirements.txt (line 9))\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wcwidth<0.3.0,>=0.2.12 (from ftfy->clip->-r requirements.txt (line 24))\n",
      "  Downloading wcwidth-0.2.12-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting httpcore (from httpx->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: sniffio in /home/ubuntu/.local/lib/python3.8/site-packages (from httpx->gradio==3.1.4->-r requirements.txt (line 20)) (1.3.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py[linkify,plugins]->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting mdit-py-plugins (from markdown-it-py[linkify,plugins]->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading mdit_py_plugins-0.4.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify,plugins]->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib->gradio==3.1.4->-r requirements.txt (line 20)) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from matplotlib->gradio==3.1.4->-r requirements.txt (line 20)) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib->gradio==3.1.4->-r requirements.txt (line 20)) (1.0.1)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets==2.4.0->datasets[vision]==2.4.0->-r requirements.txt (line 22))\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting bcrypt>=3.2 (from paramiko->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=3.3 (from paramiko->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading cryptography-41.0.5-cp37-abi3-manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting pynacl>=1.5 (from paramiko->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting exceptiongroup (from anyio<4.0.0,>=3.7.1->fastapi->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/lib/python3/dist-packages (from cryptography>=3.3->paramiko->gradio==3.1.4->-r requirements.txt (line 20)) (1.14.0)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit>=0.73.1->-r requirements.txt (line 12))\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify,plugins]->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
      "INFO: pip is looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting httpcore (from httpx->gradio==3.1.4->-r requirements.txt (line 20))\n",
      "  Downloading httpcore-1.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading httpcore-1.0.0-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading httpcore-0.18.0-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl.metadata (18 kB)\n",
      "  Downloading httpcore-0.17.2-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading httpcore-0.17.1-py3-none-any.whl.metadata (17 kB)\n",
      "  Downloading httpcore-0.17.0-py3-none-any.whl (70 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is still looking at multiple versions of httpcore to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading httpcore-0.16.2-py3-none-any.whl (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.0/69.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading httpcore-0.16.1-py3-none-any.whl (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading httpcore-0.16.0-py3-none-any.whl (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.4/68.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading streamlit-1.28.2-py2.py3-none-any.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading altair-5.1.2-py3-none-any.whl (516 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m516.2/516.2 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python_headless-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Pillow-10.1.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-14.0.1-cp38-cp38-manylinux_2_28_x86_64.whl (38.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m777.0/777.0 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.6/240.6 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading urwid-2.2.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (273 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.7/273.7 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading orjson-3.9.10-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading paramiko-3.3.1-py3-none-any.whl (224 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pycryptodome-3.19.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading cryptography-41.0.5-cp37-abi3-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wcwidth-0.2.12-py2.py3-none-any.whl (34 kB)\n",
      "Downloading yarl-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdit_py_plugins-0.4.0-py3-none-any.whl (54 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: albumentations, pudb, fire, antlr4-python3-runtime, test-tube, nomi, imgaug, ffmpy\n",
      "  Building wheel for albumentations (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for albumentations: filename=albumentations-0.4.3-py3-none-any.whl size=60766 sha256=b9c84f3e5a494cb74cf8adc0b27e23741706519b33182c62b7cbdec5b268f73d\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a0/37/4e/0bd417ba6a58f73329b825623d8c949e8e4ac2cdbd252b786d\n",
      "  Building wheel for pudb (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pudb: filename=pudb-2019.2-py3-none-any.whl size=63230 sha256=b6c621cdbdca617ee3e64cc24181ed788dc538a59195fdac91605a43b65d1dd6\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/48/83/f1/d8a09d401e2512bfda01ac9fc1b334885f9ddf51617c1c49f1\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115925 sha256=1ace5782abc2864a10fc3f570da7a2d096cb6986d88a02a08efa3ad5c304f742\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/1f/10/06/2a990ee4d73a8479fe2922445e8a876d38cfbfed052284c6a1\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=03c33efb764ad9236f8dae7b10155b639e0e7ceaf4d5330384b1f8d05ad024e8\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/c8/d0/ab/d43c02eaddc5b9004db86950802442ad9a26f279c619e28da0\n",
      "  Building wheel for test-tube (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for test-tube: filename=test_tube-0.7.5-py3-none-any.whl size=25358 sha256=60792d703be0427b7c3658310dd36c89c6bb736af1ca34bf8f58175d99c03840\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/95/b0/3a/00ea66dbb0d9ce470ce1bdcb854a6fa030c279c316cb27ca9e\n",
      "  Building editable for nomi (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nomi: filename=nomi-0.0.post1.dev2+ge9ded23-0.editable-py3-none-any.whl size=3114 sha256=42d663fbeaab0750341d100682a9210de61a14d3b0cc3b150eff3468a56f703d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-34lv69u0/wheels/5b/b5/86/b9223cc58c18a493a43af25c65699db2701c8e91bd3c2bd39b\n",
      "  Building wheel for imgaug (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for imgaug: filename=imgaug-0.2.6-py3-none-any.whl size=654018 sha256=791ebfd4efc40d0a288074f263f09cc7a3f27d78ac1d0603baf83e297d9089d7\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/41/23/e8/b1016c275f713978d312621da3c4f55920ec4297798aba8a5a\n",
      "  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5580 sha256=e4f386bbf3dd760ffb8e93f5989916f848bec08318d5679d7954cface0bb7c48\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/75/a3/1a/2f3f90b9a4eb0408109ae1b5bae01efbdf8ab4ef98797433e4\n",
      "Successfully built albumentations pudb fire antlr4-python3-runtime test-tube nomi imgaug ffmpy\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: wcwidth, tokenizers, pydub, monotonic, ffmpy, einops, braceexpand, antlr4-python3-runtime, xxhash, webdataset, watchdog, validators, urwid, urllib3, uc-micro-py, typing-extensions, toml, tenacity, taming-transformers, smmap, regex, python-multipart, pynacl, pyDeprecate, pycryptodome, pyarrow, protobuf, pillow, orjson, opencv-python-headless, opencv-python, omegaconf, multidict, mdurl, latent-diffusion, imageio-ffmpeg, h11, ftfy, fsspec, frozenlist, fire, exceptiongroup, dill, cryptography, bcrypt, backports.zoneinfo, backoff, async-timeout, yarl, uvicorn, tzlocal, torchmetrics, torch-fidelity, pydeck, pudb, paramiko, nomi, multiprocess, markdown-it-py, linkify-it-py, kornia, imgaug, imageio, gitdb, clip, anyio, aiosignal, test-tube, starlette, rich, responses, mdit-py-plugins, huggingface-hub, httpcore, gitpython, analytics-python, altair, albumentations, aiohttp, transformers, streamlit, httpx, fastapi, diffusers, pytorch-lightning, gradio, datasets\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Running setup.py develop for taming-transformers\n",
      "  Running setup.py develop for latent-diffusion\n",
      "  Running setup.py develop for clip\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 3.6.2\n",
      "    Uninstalling anyio-3.6.2:\n",
      "      Successfully uninstalled anyio-3.6.2\n",
      "Successfully installed aiohttp-3.9.0 aiosignal-1.3.1 albumentations-0.4.3 altair-5.1.2 analytics-python-1.4.post1 antlr4-python3-runtime-4.8 anyio-3.7.1 async-timeout-4.0.3 backoff-1.10.0 backports.zoneinfo-0.2.1 bcrypt-4.0.1 braceexpand-0.1.7 clip cryptography-41.0.5 datasets-2.4.0 diffusers-0.3.0 dill-0.3.5.1 einops-0.3.0 exceptiongroup-1.2.0 fastapi-0.104.1 ffmpy-0.3.1 fire-0.4.0 frozenlist-1.4.0 fsspec-2023.10.0 ftfy-6.1.3 gitdb-4.0.11 gitpython-3.1.40 gradio-3.1.4 h11-0.12.0 httpcore-0.15.0 httpx-0.25.1 huggingface-hub-0.19.4 imageio-2.9.0 imageio-ffmpeg-0.4.2 imgaug-0.2.6 kornia-0.6.0 latent-diffusion linkify-it-py-2.0.2 markdown-it-py-3.0.0 mdit-py-plugins-0.4.0 mdurl-0.1.2 monotonic-1.6 multidict-6.0.4 multiprocess-0.70.13 nomi-0.0.post1.dev2+ge9ded23 omegaconf-2.1.1 opencv-python-4.5.5.64 opencv-python-headless-4.8.1.78 orjson-3.9.10 paramiko-3.3.1 pillow-10.1.0 protobuf-4.25.1 pudb-2019.2 pyDeprecate-0.3.1 pyarrow-14.0.1 pycryptodome-3.19.0 pydeck-0.8.1b0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.6 pytorch-lightning-1.4.2 regex-2023.10.3 responses-0.18.0 rich-13.7.0 smmap-5.0.1 starlette-0.27.0 streamlit-1.28.2 taming-transformers tenacity-8.2.3 test-tube-0.7.5 tokenizers-0.12.1 toml-0.10.2 torch-fidelity-0.3.0 torchmetrics-0.6.0 transformers-4.22.2 typing-extensions-4.8.0 tzlocal-5.2 uc-micro-py-1.0.2 urllib3-1.26.18 urwid-2.2.3 uvicorn-0.24.0.post1 validators-0.22.0 watchdog-3.0.0 wcwidth-0.2.12 webdataset-0.2.5 xxhash-3.4.1 yarl-1.9.3\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/justinpinkney/stable-diffusion.git\n",
    "%cd stable-diffusion\n",
    "!pip install --upgrade pip\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "#!pip install --upgrade keras # on lambda stack we need to upgrade keras\n",
    "#!pip uninstall -y torchtext # on colab we need to remove torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting typing-extensions==4.5.0\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: typing-extensions\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastapi 0.104.1 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed typing-extensions-4.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install typing-extensions==4.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "m1AkWL270DSE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Nov 22 22:40:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA RTX A6000    On   | 00000000:05:00.0 Off |                  Off |\n",
      "| 30%   30C    P8     6W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000    On   | 00000000:06:00.0 Off |                  Off |\n",
      "| 30%   29C    P8     6W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000    On   | 00000000:07:00.0 Off |                  Off |\n",
      "| 30%   29C    P8     5W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000    On   | 00000000:08:00.0 Off |                  Off |\n",
      "| 30%   29C    P8    10W / 300W |      1MiB / 49140MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PbMtzkytz8kI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: huggingface_hub in /home/ubuntu/.local/lib/python3.8/site-packages (0.19.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub) (3.0.12)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface_hub) (5.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.9->huggingface_hub) (2.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->huggingface_hub) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->huggingface_hub) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub) (2019.11.28)\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200c9ddfea71409dbdfa5a8163bf91f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install huggingface_hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FTf_OdfEz8kI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset parquet/msklar--skribbl-drawings to /home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df9a0299f5f6470e97fbde9dc574958c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53ff95866c414a92aa4e6a1652efd4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/796k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5982c69088c442da7cf454d79cb713a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "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\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=256x256>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alien\n"
     ]
    }
   ],
   "source": [
    "# Check the dataset\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"msklar/skribbl-drawings\", split='train')\n",
    "\n",
    "sample = ds[0]\n",
    "display(sample[\"image\"].resize((256, 256)))\n",
    "print(sample[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V1uUVjQz8kI"
   },
   "source": [
    "To get the weights you need to you'll need to [go to the model card](https://huggingface.co/CompVis/stable-diffusion-v1-4-original), read the license and tick the checkbox if you agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "vnnbNNycz8kI"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e42c78a5674fdeb7217545768a7d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sd-v1-4-full-ema.ckpt:   0%|          | 0.00/7.70G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "ckpt_path = hf_hub_download(repo_id=\"CompVis/stable-diffusion-v-1-4-original\", filename=\"sd-v1-4-full-ema.ckpt\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8VDyuQxz8kJ"
   },
   "source": [
    "Set your parameters below depending on your GPU setup, the settings below were used for training on a 2xA6000 machine, (the A6000 has 48GB of VRAM). On this set up good results are achieved in around 6 hours.\n",
    "\n",
    "You can make up for using smaller batches or fewer gpus by accumulating batches:\n",
    "\n",
    "`total batch size = batach size * n gpus * accumulate batches`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WVssEQJfz8kJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPUs: 0,1,2,3,\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 1xA100:\n",
    "BATCH_SIZE = 4\n",
    "N_GPUS = 1\n",
    "ACCUMULATE_BATCHES = 1\n",
    "\"\"\"\n",
    "\n",
    "# 4xA6000:\n",
    "BATCH_SIZE = 4\n",
    "N_GPUS = 4\n",
    "ACCUMULATE_BATCHES = 1\n",
    "\n",
    "gpu_list=\"0\"\n",
    "\n",
    "if N_GPUS > 1:\n",
    "  gpu_list = \",\".join((str(x) for x in range(N_GPUS))) + \",\"\n",
    "else:\n",
    "  gpu_list = \"0\"\n",
    "\n",
    "print(f\"Using GPUs: {gpu_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python==4.8.0.74\n",
      "  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from opencv-python==4.8.0.74) (1.23.4)\n",
      "Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: opencv-python\n",
      "  Attempting uninstall: opencv-python\n",
      "    Found existing installation: opencv-python 4.5.5.64\n",
      "    Uninstalling opencv-python-4.5.5.64:\n",
      "      Successfully uninstalled opencv-python-4.5.5.64\n",
      "Successfully installed opencv-python-4.8.0.74\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/arogozhnikov/einops.git\n",
      "  Cloning https://github.com/arogozhnikov/einops.git to /tmp/pip-req-build-oumxtw9u\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/arogozhnikov/einops.git /tmp/pip-req-build-oumxtw9u\n",
      "  Resolved https://github.com/arogozhnikov/einops.git to commit a6e93530ec2dce44f473e6065fad4d8236cda4f3\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: einops\n",
      "  Building wheel for einops (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for einops: filename=einops-0.7.0-py3-none-any.whl size=44539 sha256=da1f06dad08bb907e963178dfabd312a82abcafaf1083b280c8cc1c802675f18\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-v0lu7wg_/wheels/c3/3e/b2/013ce57c10a5d46d881dccba905dd1eae452037d41c99df8d1\n",
      "Successfully built einops\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: einops\n",
      "  Attempting uninstall: einops\n",
      "    Found existing installation: einops 0.3.0\n",
      "    Uninstalling einops-0.3.0:\n",
      "      Successfully uninstalled einops-0.3.0\n",
      "Successfully installed einops-0.7.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Pillow==9.5.0\n",
      "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: Pillow\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 10.1.0\n",
      "    Uninstalling Pillow-10.1.0:\n",
      "      Successfully uninstalled Pillow-10.1.0\n",
      "Successfully installed Pillow-9.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python==4.8.0.74\n",
    "!pip install --upgrade --force-reinstall git+https://github.com/arogozhnikov/einops.git\n",
    "!pip install Pillow==9.5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting protobuf==3.20.*\n",
      "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.1\n",
      "    Uninstalling protobuf-4.25.1:\n",
      "      Successfully uninstalled protobuf-4.25.1\n",
      "Successfully installed protobuf-3.20.3\n"
     ]
    }
   ],
   "source": [
    "# if a protobuf warning pops up in the following cell, run this:\n",
    "#!pip install protobuf==3.20.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "w7sLO53fz8kJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Global seed set to 23\n",
      "Running on GPUs 0,1,2,3,\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'text_projection.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'logit_scale', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.2.layer_norm2.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Attempting to load state from /home/ubuntu/.cache/huggingface/hub/models--CompVis--stable-diffusion-v-1-4-original/snapshots/f0bb45b49990512c454cf2c5670b0952ef2f9c71/sd-v1-4-full-ema.ckpt\n",
      "Found nested key 'state_dict' in checkpoint, loading this instead\n",
      "Monitoring val/loss as checkpoint metric.\n",
      "Merged modelckpt-cfg: \n",
      "{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': '../checkpoints/', 'filename': '{epoch}-{step}', 'verbose': True, 'save_last': True, 'monitor': 'train/loss', 'save_top_k': -1, 'every_n_epochs': 15}}\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory ../checkpoints/ exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "/usr/lib/python3/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "#### Data #####\n",
      "train, Dataset, 232\n",
      "validation, TextOnly, 16\n",
      "accumulate_grad_batches = 1\n",
      "++++ NOT USING LR SCALING ++++\n",
      "Setting learning rate to 1.00e-04\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Global seed set to 23\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Global seed set to 23\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Global seed set to 23\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'visual_projection.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'logit_scale', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'text_projection.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.embeddings.position_ids', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Global seed set to 23\n",
      "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'logit_scale', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'visual_projection.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.bias']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "/usr/lib/python3/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Global seed set to 23\n",
      "initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "/usr/lib/python3/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Global seed set to 23\n",
      "initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.6.layer_norm2.weight', 'text_projection.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'logit_scale', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "/usr/lib/python3/dist-packages/torchvision/transforms/transforms.py:332: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Using custom data configuration msklar--skribbl-drawings-ef5b50e1518e7dbc\n",
      "Reusing dataset parquet (/home/ubuntu/.cache/huggingface/datasets/msklar___parquet/msklar--skribbl-drawings-ef5b50e1518e7dbc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Global seed set to 23\n",
      "initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All DDP processes registered. Starting ddp with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:423: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "Training the full unet\n",
      "Training the full unet\n",
      "Training the full unet\n",
      "Training the full unet\n",
      "Setting up LambdaLR scheduler...\n",
      "Setting up LambdaLR scheduler...\n",
      "Setting up LambdaLR scheduler...\n",
      "Setting up LambdaLR scheduler...\n",
      "/usr/lib/python3/dist-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/lib/python3/dist-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "Project config\n",
      "model:\n",
      "  base_learning_rate: 0.0001\n",
      "  target: ldm.models.diffusion.ddpm.LatentDiffusion\n",
      "  params:\n",
      "    linear_start: 0.00085\n",
      "    linear_end: 0.012\n",
      "    num_timesteps_cond: 1\n",
      "    log_every_t: 200\n",
      "    timesteps: 1000\n",
      "    first_stage_key: image\n",
      "    cond_stage_key: txt\n",
      "    image_size: 64\n",
      "    channels: 4\n",
      "    cond_stage_trainable: false\n",
      "    conditioning_key: crossattn\n",
      "    scale_factor: 0.18215\n",
      "    scheduler_config:\n",
      "      target: ldm.lr_scheduler.LambdaLinearScheduler\n",
      "      params:\n",
      "        warm_up_steps:\n",
      "        - 1\n",
      "        cycle_lengths:\n",
      "        - 10000000000000\n",
      "        f_start:\n",
      "        - 1.0e-06\n",
      "        f_max:\n",
      "        - 1.0\n",
      "        f_min:\n",
      "        - 1.0\n",
      "    unet_config:\n",
      "      target: ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
      "      params:\n",
      "        image_size: 32\n",
      "        in_channels: 4\n",
      "        out_channels: 4\n",
      "        model_channels: 320\n",
      "        attention_resolutions:\n",
      "        - 4\n",
      "        - 2\n",
      "        - 1\n",
      "        num_res_blocks: 2\n",
      "        channel_mult:\n",
      "        - 1\n",
      "        - 2\n",
      "        - 4\n",
      "        - 4\n",
      "        num_heads: 8\n",
      "        use_spatial_transformer: true\n",
      "        transformer_depth: 1\n",
      "        context_dim: 768\n",
      "        use_checkpoint: true\n",
      "        legacy: false\n",
      "    first_stage_config:\n",
      "      target: ldm.models.autoencoder.AutoencoderKL\n",
      "      ckpt_path: models/first_stage_models/kl-f8/model.ckpt\n",
      "      params:\n",
      "        embed_dim: 4\n",
      "        monitor: val/rec_loss\n",
      "        ddconfig:\n",
      "          double_z: true\n",
      "          z_channels: 4\n",
      "          resolution: 256\n",
      "          in_channels: 3\n",
      "          out_ch: 3\n",
      "          ch: 128\n",
      "          ch_mult:\n",
      "          - 1\n",
      "          - 2\n",
      "          - 4\n",
      "          - 4\n",
      "          num_res_blocks: 2\n",
      "          attn_resolutions: []\n",
      "          dropout: 0.0\n",
      "        lossconfig:\n",
      "          target: torch.nn.Identity\n",
      "    cond_stage_config:\n",
      "      target: ldm.modules.encoders.modules.FrozenCLIPEmbedder\n",
      "data:\n",
      "  target: main.DataModuleFromConfig\n",
      "  params:\n",
      "    batch_size: 4\n",
      "    num_workers: 56\n",
      "    num_val_workers: 0\n",
      "    train:\n",
      "      target: ldm.data.simple.hf_dataset\n",
      "      params:\n",
      "        name: msklar/skribbl-drawings\n",
      "        image_transforms:\n",
      "        - target: torchvision.transforms.Resize\n",
      "          params:\n",
      "            size: 512\n",
      "            interpolation: 3\n",
      "        - target: torchvision.transforms.RandomCrop\n",
      "          params:\n",
      "            size: 512\n",
      "        - target: torchvision.transforms.RandomHorizontalFlip\n",
      "    validation:\n",
      "      target: ldm.data.simple.TextOnly\n",
      "      params:\n",
      "        captions:\n",
      "        - apple\n",
      "        - smile\n",
      "        - duck\n",
      "        - hospital\n",
      "        output_size: 512\n",
      "        n_gpus: 4\n",
      "\n",
      "Lightning config\n",
      "find_unused_parameters: false\n",
      "modelcheckpoint:\n",
      "  params:\n",
      "    every_n_epochs: 15\n",
      "    save_top_k: -1\n",
      "    save_last: true\n",
      "    dirpath: ../checkpoints/\n",
      "    filename: '{epoch}-{step}'\n",
      "    verbose: true\n",
      "    monitor: train/loss\n",
      "callbacks:\n",
      "  image_logger:\n",
      "    target: main.ImageLogger\n",
      "    params:\n",
      "      batch_frequency: 2000\n",
      "      max_images: 4\n",
      "      increase_log_steps: false\n",
      "      log_first_step: true\n",
      "      log_all_val: true\n",
      "      log_images_kwargs:\n",
      "        use_ema_scope: true\n",
      "        inpaint: false\n",
      "        plot_progressive_rows: false\n",
      "        plot_diffusion_rows: false\n",
      "        'N': 4\n",
      "        unconditional_guidance_scale: 3.0\n",
      "        unconditional_guidance_label:\n",
      "        - ''\n",
      "trainer:\n",
      "  benchmark: true\n",
      "  num_sanity_val_steps: 0\n",
      "  accumulate_grad_batches: 1\n",
      "  accelerator: ddp\n",
      "  check_val_every_n_epoch: 10\n",
      "  gpus: 0,1,2,3,\n",
      "\n",
      "\n",
      "  | Name              | Type               | Params\n",
      "---------------------------------------------------------\n",
      "0 | model             | DiffusionWrapper   | 859 M \n",
      "1 | model_ema         | LitEma             | 0     \n",
      "2 | first_stage_model | AutoencoderKL      | 83.7 M\n",
      "3 | cond_stage_model  | FrozenCLIPEmbedder | 123 M \n",
      "---------------------------------------------------------\n",
      "859 M     Trainable params\n",
      "206 M     Non-trainable params\n",
      "1.1 B     Total params\n",
      "4,264.941 Total estimated model params size (MB)\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 56 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Epoch 0:   0%|                                 | 0/15 [00:00<00:00, 6626.07it/s]Sampling: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 199 timesteps\n",
      "DDIM Sampler:   0%|                                     | 0/199 [00:00<?, ?it/s]Sampling: Switched to EMA weights\n",
      "Sampling: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 199 timesteps\n",
      "DDIM Sampler:   0%|                                     | 0/199 [00:00<?, ?it/s]Data shape for DDIM sampling is (4, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 199 timesteps\n",
      "DDIM Sampler:   0%|                                     | 0/199 [00:00<?, ?it/s]Sampling: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 199 timesteps\n",
      "\n",
      "DDIM Sampler:   1%|▏                            | 1/199 [00:00<00:51,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   1%|▎                            | 2/199 [00:00<00:50,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▍                            | 3/199 [00:00<00:50,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   2%|▌                            | 4/199 [00:01<00:50,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   3%|▋                            | 5/199 [00:01<00:50,  3.86it/s]\u001b[A\n",
      "DDIM Sampler:   3%|▊                            | 6/199 [00:01<00:49,  3.86it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█                            | 7/199 [00:01<00:49,  3.86it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                           | 8/199 [00:02<00:49,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   5%|█▎                           | 9/199 [00:02<00:49,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   5%|█▍                          | 10/199 [00:02<00:48,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▌                          | 11/199 [00:02<00:48,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▋                          | 12/199 [00:03<00:48,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   7%|█▊                          | 13/199 [00:03<00:48,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   7%|█▉                          | 14/199 [00:03<00:47,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██                          | 15/199 [00:03<00:47,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▎                         | 16/199 [00:04<00:47,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   9%|██▍                         | 17/199 [00:04<00:46,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:   9%|██▌                         | 18/199 [00:04<00:46,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  10%|██▋                         | 19/199 [00:04<00:46,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  10%|██▊                         | 20/199 [00:05<00:46,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  11%|██▉                         | 21/199 [00:05<00:45,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  11%|███                         | 22/199 [00:05<00:45,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▏                        | 23/199 [00:05<00:45,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▍                        | 24/199 [00:06<00:45,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  13%|███▌                        | 25/199 [00:06<00:44,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  13%|███▋                        | 26/199 [00:06<00:44,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  14%|███▊                        | 27/199 [00:06<00:44,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  14%|███▉                        | 28/199 [00:07<00:44,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  15%|████                        | 29/199 [00:07<00:43,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  15%|████▏                       | 30/199 [00:07<00:43,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▎                       | 31/199 [00:08<00:43,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▌                       | 32/199 [00:08<00:43,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  17%|████▋                       | 33/199 [00:08<00:42,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  17%|████▊                       | 34/199 [00:08<00:42,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  18%|████▉                       | 35/199 [00:09<00:42,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████                       | 36/199 [00:09<00:42,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  19%|█████▏                      | 37/199 [00:09<00:41,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  19%|█████▎                      | 38/199 [00:09<00:41,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▍                      | 39/199 [00:10<00:41,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▋                      | 40/199 [00:10<00:41,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  21%|█████▊                      | 41/199 [00:10<00:40,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  21%|█████▉                      | 42/199 [00:10<00:40,  3.88it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████                      | 43/199 [00:11<00:40,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▏                     | 44/199 [00:11<00:40,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  23%|██████▎                     | 45/199 [00:11<00:39,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  23%|██████▍                     | 46/199 [00:11<00:39,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▌                     | 47/199 [00:12<00:39,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  25%|██████▉                     | 49/199 [00:12<00:38,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  25%|███████                     | 50/199 [00:12<00:38,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▏                    | 51/199 [00:13<00:37,  3.91it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▎                    | 52/199 [00:13<00:37,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  27%|███████▍                    | 53/199 [00:13<00:37,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  27%|███████▌                    | 54/199 [00:13<00:37,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  28%|███████▋                    | 55/199 [00:14<00:36,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  28%|███████▉                    | 56/199 [00:14<00:36,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  29%|████████                    | 57/199 [00:14<00:36,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  29%|████████▏                   | 58/199 [00:14<00:36,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▎                   | 59/199 [00:15<00:35,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▍                   | 60/199 [00:15<00:35,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  31%|████████▌                   | 61/199 [00:15<00:35,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  31%|████████▋                   | 62/199 [00:15<00:35,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  32%|████████▊                   | 63/199 [00:16<00:34,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████                   | 64/199 [00:16<00:34,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  33%|█████████▏                  | 65/199 [00:16<00:34,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  33%|█████████▎                  | 66/199 [00:16<00:34,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▍                  | 67/199 [00:17<00:33,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▌                  | 68/199 [00:17<00:33,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  35%|█████████▋                  | 69/199 [00:17<00:33,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  35%|█████████▊                  | 70/199 [00:17<00:33,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  36%|█████████▉                  | 71/199 [00:18<00:32,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  36%|██████████▏                 | 72/199 [00:18<00:32,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  37%|██████████▎                 | 73/199 [00:18<00:32,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  37%|██████████▍                 | 74/199 [00:18<00:32,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  38%|██████████▌                 | 75/199 [00:19<00:31,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  38%|██████████▋                 | 76/199 [00:19<00:31,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  39%|██████████▊                 | 77/199 [00:19<00:31,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  39%|██████████▉                 | 78/199 [00:19<00:31,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████                 | 79/199 [00:20<00:30,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▎                | 80/199 [00:20<00:30,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  41%|███████████▍                | 81/199 [00:20<00:30,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  41%|███████████▌                | 82/199 [00:20<00:30,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  42%|███████████▋                | 83/199 [00:21<00:29,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  42%|███████████▊                | 84/199 [00:21<00:29,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  43%|███████████▉                | 85/199 [00:21<00:29,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  43%|████████████                | 86/199 [00:22<00:29,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▏               | 87/199 [00:22<00:28,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▍               | 88/199 [00:22<00:28,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  45%|████████████▌               | 89/199 [00:22<00:28,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  45%|████████████▋               | 90/199 [00:23<00:27,  3.89it/s]\u001b[A\n",
      "DDIM Sampler:  46%|████████████▊               | 91/199 [00:23<00:27,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  46%|████████████▉               | 92/199 [00:23<00:27,  3.90it/s]\u001b[A\n",
      "DDIM Sampler:  46%|████████████▉               | 92/199 [00:23<00:27,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  47%|█████████████               | 93/199 [00:24<00:27,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  47%|█████████████▏              | 94/199 [00:24<00:27,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▎              | 95/199 [00:24<00:27,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▌              | 96/199 [00:24<00:26,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  49%|█████████████▋              | 97/199 [00:25<00:26,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  49%|█████████████▊              | 98/199 [00:25<00:26,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  50%|█████████████▉              | 99/199 [00:25<00:25,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  50%|█████████████▌             | 100/199 [00:25<00:25,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  51%|█████████████▋             | 101/199 [00:26<00:25,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  51%|█████████████▊             | 102/199 [00:26<00:25,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  52%|█████████████▉             | 103/199 [00:26<00:24,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  52%|██████████████             | 104/199 [00:26<00:24,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  53%|██████████████▏            | 105/199 [00:27<00:24,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  53%|██████████████▍            | 106/199 [00:27<00:24,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  54%|██████████████▌            | 107/199 [00:27<00:23,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  54%|██████████████▋            | 108/199 [00:27<00:23,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  55%|██████████████▊            | 109/199 [00:28<00:23,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  55%|██████████████▉            | 110/199 [00:28<00:23,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  56%|███████████████            | 111/199 [00:28<00:22,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  56%|███████████████▏           | 112/199 [00:28<00:22,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  57%|███████████████▎           | 113/199 [00:29<00:22,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  57%|███████████████▍           | 114/199 [00:29<00:22,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  58%|███████████████▌           | 115/199 [00:29<00:21,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  58%|███████████████▋           | 116/199 [00:30<00:21,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  59%|███████████████▊           | 117/199 [00:30<00:21,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  59%|████████████████           | 118/199 [00:30<00:21,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  60%|████████████████▏          | 119/199 [00:30<00:20,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  60%|████████████████▎          | 120/199 [00:31<00:20,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  61%|████████████████▍          | 121/199 [00:31<00:20,  3.85it/s]\u001b[A\n",
      "DDIM Sampler:  61%|████████████████▌          | 122/199 [00:31<00:20,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  62%|████████████████▋          | 123/199 [00:31<00:19,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  62%|████████████████▊          | 124/199 [00:32<00:19,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  63%|████████████████▉          | 125/199 [00:32<00:19,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  63%|█████████████████          | 126/199 [00:32<00:18,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  64%|█████████████████▏         | 127/199 [00:32<00:18,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  64%|█████████████████▎         | 128/199 [00:33<00:18,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  65%|█████████████████▌         | 129/199 [00:33<00:18,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  65%|█████████████████▋         | 130/199 [00:33<00:17,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  66%|█████████████████▊         | 131/199 [00:33<00:17,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  66%|█████████████████▉         | 132/199 [00:34<00:17,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  67%|██████████████████         | 133/199 [00:34<00:17,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  67%|██████████████████▏        | 134/199 [00:34<00:16,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  68%|██████████████████▎        | 135/199 [00:34<00:16,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  68%|██████████████████▍        | 136/199 [00:35<00:16,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  69%|██████████████████▌        | 137/199 [00:35<00:16,  3.84it/s]\u001b[A\n",
      "DDIM Sampler:  69%|██████████████████▋        | 138/199 [00:35<00:15,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  70%|██████████████████▊        | 139/199 [00:36<00:15,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  70%|██████████████████▉        | 140/199 [00:36<00:15,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  71%|███████████████████▏       | 141/199 [00:36<00:15,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  71%|███████████████████▎       | 142/199 [00:36<00:14,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  72%|███████████████████▍       | 143/199 [00:37<00:14,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  72%|███████████████████▌       | 144/199 [00:37<00:14,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  73%|███████████████████▋       | 145/199 [00:37<00:14,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  73%|███████████████████▊       | 146/199 [00:37<00:13,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  74%|███████████████████▉       | 147/199 [00:38<00:13,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  74%|████████████████████       | 148/199 [00:38<00:13,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  75%|████████████████████▏      | 149/199 [00:38<00:13,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  75%|████████████████████▎      | 150/199 [00:38<00:12,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  76%|████████████████████▍      | 151/199 [00:39<00:12,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  76%|████████████████████▌      | 152/199 [00:39<00:12,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  77%|████████████████████▊      | 153/199 [00:39<00:12,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  77%|████████████████████▉      | 154/199 [00:39<00:11,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  78%|█████████████████████      | 155/199 [00:40<00:11,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  78%|█████████████████████▏     | 156/199 [00:40<00:11,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  79%|█████████████████████▎     | 157/199 [00:40<00:10,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  79%|█████████████████████▍     | 158/199 [00:40<00:10,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  80%|█████████████████████▌     | 159/199 [00:41<00:10,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  80%|█████████████████████▋     | 160/199 [00:41<00:10,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  81%|█████████████████████▊     | 161/199 [00:41<00:09,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  81%|█████████████████████▉     | 162/199 [00:42<00:09,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  82%|██████████████████████     | 163/199 [00:42<00:09,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  82%|██████████████████████▎    | 164/199 [00:42<00:09,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  83%|██████████████████████▍    | 165/199 [00:42<00:08,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  84%|██████████████████████▊    | 168/199 [00:43<00:08,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  85%|██████████████████████▉    | 169/199 [00:43<00:07,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  85%|███████████████████████    | 170/199 [00:43<00:07,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  86%|███████████████████████▏   | 171/199 [00:43<00:07,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  86%|███████████████████████▎   | 172/199 [00:44<00:06,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  87%|███████████████████████▍   | 173/199 [00:44<00:06,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  87%|███████████████████████▌   | 174/199 [00:44<00:06,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  88%|███████████████████████▋   | 175/199 [00:44<00:06,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  88%|███████████████████████▉   | 176/199 [00:45<00:05,  3.87it/s]\u001b[A\n",
      "DDIM Sampler:  88%|███████████████████████▋   | 175/199 [00:45<00:06,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  88%|███████████████████████▉   | 176/199 [00:45<00:06,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  89%|████████████████████████   | 177/199 [00:45<00:05,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  89%|████████████████████████▏  | 178/199 [00:46<00:05,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  90%|████████████████████████▎  | 179/199 [00:46<00:05,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  90%|████████████████████████▍  | 180/199 [00:46<00:04,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  91%|████████████████████████▌  | 181/199 [00:46<00:04,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  91%|████████████████████████▋  | 182/199 [00:47<00:04,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  92%|████████████████████████▊  | 183/199 [00:47<00:04,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  92%|████████████████████████▉  | 184/199 [00:47<00:03,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  93%|█████████████████████████  | 185/199 [00:48<00:03,  3.83it/s]\u001b[A\n",
      "DDIM Sampler:  93%|█████████████████████████▏ | 186/199 [00:48<00:03,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  94%|█████████████████████████▎ | 187/199 [00:48<00:03,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  94%|█████████████████████████▌ | 188/199 [00:48<00:02,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  95%|█████████████████████████▋ | 189/199 [00:49<00:02,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  95%|█████████████████████████▊ | 190/199 [00:49<00:02,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  96%|█████████████████████████▉ | 191/199 [00:49<00:02,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  96%|██████████████████████████ | 192/199 [00:49<00:01,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  97%|██████████████████████████▏| 193/199 [00:50<00:01,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  97%|██████████████████████████▎| 194/199 [00:50<00:01,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  98%|██████████████████████████▍| 195/199 [00:50<00:01,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  98%|██████████████████████████▌| 196/199 [00:50<00:00,  3.82it/s]\u001b[A\n",
      "DDIM Sampler: 100%|███████████████████████████| 199/199 [00:51<00:00,  3.89it/s]\u001b[A\n",
      "Sampling: Restored training weights\n",
      "DDIM Sampler:  99%|██████████████████████████▋| 197/199 [00:51<00:00,  3.82it/s]\n",
      "DDIM Sampler:  99%|██████████████████████████▊| 198/199 [00:51<00:00,  3.82it/s]\u001b[A\n",
      "DDIM Sampler:  99%|██████████████████████████▊| 198/199 [00:51<00:00,  3.83it/s]\u001b[ASampling with classifier-free guidance: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 199 timesteps\n",
      "DDIM Sampler: 100%|███████████████████████████| 199/199 [00:51<00:00,  3.85it/s]\n",
      "Sampling: Restored training weights\n",
      "DDIM Sampler: 100%|███████████████████████████| 199/199 [00:51<00:00,  3.85it/s]\n",
      "\n",
      "DDIM Sampler: 100%|███████████████████████████| 199/199 [00:51<00:00,  3.86it/s]\u001b[A\n",
      "Sampling: Restored training weights\n",
      "Sampling: Restored training weights\n",
      "Sampling with classifier-free guidance: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 199 timesteps\n",
      "DDIM Sampler:   0%|                                     | 0/199 [00:00<?, ?it/s]Sampling with classifier-free guidance: Switched to EMA weights\n",
      "Sampling with classifier-free guidance: Switched to EMA weights\n",
      "Data shape for DDIM sampling is (4, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 199 timesteps\n",
      "DDIM Sampler:   0%|                                     | 0/199 [00:00<?, ?it/s]Data shape for DDIM sampling is (4, 4, 64, 64), eta 1.0\n",
      "Running DDIM Sampling with 199 timesteps\n",
      "\n",
      "DDIM Sampler:   1%|▏                            | 1/199 [00:06<22:49,  6.92s/it]\u001b[A\n",
      "DDIM Sampler:   2%|▌                            | 4/199 [00:08<04:22,  1.35s/it]\u001b[A\n",
      "DDIM Sampler:   3%|▋                            | 5/199 [00:08<03:22,  1.04s/it]\u001b[A\n",
      "DDIM Sampler:   3%|▊                            | 6/199 [00:09<02:45,  1.17it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█                            | 7/199 [00:09<02:22,  1.35it/s]\u001b[A\n",
      "DDIM Sampler:   4%|█▏                           | 8/199 [00:10<02:07,  1.50it/s]\u001b[A\n",
      "DDIM Sampler:   5%|█▎                           | 9/199 [00:10<01:56,  1.63it/s]\u001b[A\n",
      "DDIM Sampler:   5%|█▍                          | 10/199 [00:11<01:49,  1.73it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▌                          | 11/199 [00:11<01:44,  1.80it/s]\u001b[A\n",
      "DDIM Sampler:   6%|█▋                          | 12/199 [00:12<01:41,  1.85it/s]\u001b[A\n",
      "DDIM Sampler:   7%|█▊                          | 13/199 [00:12<01:38,  1.89it/s]\u001b[A\n",
      "DDIM Sampler:   7%|█▉                          | 14/199 [00:13<01:36,  1.92it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██                          | 15/199 [00:13<01:34,  1.94it/s]\u001b[A\n",
      "DDIM Sampler:   8%|██▎                         | 16/199 [00:14<01:33,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:   9%|██▍                         | 17/199 [00:14<01:32,  1.97it/s]\u001b[A\n",
      "DDIM Sampler:   9%|██▌                         | 18/199 [00:15<01:31,  1.97it/s]\u001b[A\n",
      "DDIM Sampler:  10%|██▋                         | 19/199 [00:15<01:31,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  10%|██▊                         | 20/199 [00:16<01:30,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  11%|██▉                         | 21/199 [00:16<01:29,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  11%|███                         | 22/199 [00:17<01:29,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▏                        | 23/199 [00:17<01:28,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  12%|███▍                        | 24/199 [00:18<01:28,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  13%|███▌                        | 25/199 [00:18<01:27,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  13%|███▋                        | 26/199 [00:19<01:26,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  14%|███▊                        | 27/199 [00:19<01:26,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  14%|███▉                        | 28/199 [00:20<01:25,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  15%|████                        | 29/199 [00:20<01:25,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  15%|████▏                       | 30/199 [00:21<01:24,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▎                       | 31/199 [00:21<01:24,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  16%|████▌                       | 32/199 [00:22<01:23,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  17%|████▋                       | 33/199 [00:22<01:23,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  17%|████▊                       | 34/199 [00:23<01:22,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  18%|████▉                       | 35/199 [00:23<01:22,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  18%|█████                       | 36/199 [00:24<01:22,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  19%|█████▏                      | 37/199 [00:24<01:21,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  19%|█████▎                      | 38/199 [00:25<01:20,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▍                      | 39/199 [00:25<01:20,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  20%|█████▋                      | 40/199 [00:26<01:19,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  21%|█████▊                      | 41/199 [00:26<01:19,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  21%|█████▉                      | 42/199 [00:27<01:18,  1.99it/s]\u001b[A\n",
      "DDIM Sampler:  21%|█████▊                      | 41/199 [00:27<01:20,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  21%|█████▉                      | 42/199 [00:27<01:20,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████                      | 43/199 [00:28<01:19,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  22%|██████▏                     | 44/199 [00:28<01:18,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  23%|██████▎                     | 45/199 [00:29<01:18,  1.97it/s]\u001b[A\n",
      "DDIM Sampler:  23%|██████▍                     | 46/199 [00:29<01:17,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▌                     | 47/199 [00:30<01:17,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  24%|██████▊                     | 48/199 [00:30<01:17,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  25%|██████▉                     | 49/199 [00:31<01:16,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  25%|███████                     | 50/199 [00:31<01:15,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▏                    | 51/199 [00:32<01:15,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  26%|███████▎                    | 52/199 [00:32<01:14,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  27%|███████▍                    | 53/199 [00:33<01:14,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  27%|███████▌                    | 54/199 [00:33<01:14,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  28%|███████▋                    | 55/199 [00:34<01:13,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  28%|███████▉                    | 56/199 [00:34<01:13,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  29%|████████                    | 57/199 [00:35<01:12,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  29%|████████▏                   | 58/199 [00:35<01:12,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▎                   | 59/199 [00:36<01:11,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  30%|████████▍                   | 60/199 [00:36<01:11,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  31%|████████▌                   | 61/199 [00:37<01:10,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  31%|████████▋                   | 62/199 [00:37<01:09,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  32%|████████▊                   | 63/199 [00:38<01:09,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  32%|█████████                   | 64/199 [00:38<01:09,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  33%|█████████▏                  | 65/199 [00:39<01:08,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  33%|█████████▎                  | 66/199 [00:39<01:08,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▍                  | 67/199 [00:40<01:07,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  34%|█████████▌                  | 68/199 [00:40<01:07,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  35%|█████████▋                  | 69/199 [00:41<01:06,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  35%|█████████▊                  | 70/199 [00:42<01:06,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  36%|█████████▉                  | 71/199 [00:42<01:05,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  37%|██████████▍                 | 74/199 [00:43<01:03,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  38%|██████████▌                 | 75/199 [00:43<01:02,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  38%|██████████▋                 | 76/199 [00:44<01:02,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  39%|██████████▊                 | 77/199 [00:44<01:01,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  39%|██████████▉                 | 78/199 [00:45<01:01,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████                 | 79/199 [00:45<01:00,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▎                | 80/199 [00:46<01:00,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  41%|███████████▍                | 81/199 [00:46<00:59,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████                 | 79/199 [00:46<01:01,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  40%|███████████▎                | 80/199 [00:47<01:00,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  41%|███████████▍                | 81/199 [00:47<01:00,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  41%|███████████▌                | 82/199 [00:48<00:59,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  42%|███████████▋                | 83/199 [00:48<00:59,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  42%|███████████▊                | 84/199 [00:49<00:58,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  43%|███████████▉                | 85/199 [00:49<00:58,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  43%|████████████                | 86/199 [00:50<00:57,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▏               | 87/199 [00:50<00:57,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  44%|████████████▍               | 88/199 [00:51<00:56,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  45%|████████████▌               | 89/199 [00:51<00:56,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  45%|████████████▋               | 90/199 [00:52<00:55,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  46%|████████████▊               | 91/199 [00:52<00:55,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  46%|████████████▉               | 92/199 [00:53<00:54,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  47%|█████████████               | 93/199 [00:53<00:54,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  47%|█████████████▏              | 94/199 [00:54<00:53,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▎              | 95/199 [00:54<00:53,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  48%|█████████████▌              | 96/199 [00:55<00:52,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  49%|█████████████▋              | 97/199 [00:56<00:52,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  49%|█████████████▊              | 98/199 [00:56<00:51,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  50%|█████████████▉              | 99/199 [00:57<00:51,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  50%|█████████████▌             | 100/199 [00:57<00:50,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  51%|█████████████▋             | 101/199 [00:58<00:50,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  51%|█████████████▊             | 102/199 [00:58<00:49,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  52%|█████████████▉             | 103/199 [00:59<00:49,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  52%|██████████████             | 104/199 [00:59<00:48,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  53%|██████████████▏            | 105/199 [01:00<00:48,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  53%|██████████████▍            | 106/199 [01:00<00:47,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  54%|██████████████▌            | 107/199 [01:01<00:47,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  54%|██████████████▋            | 108/199 [01:01<00:46,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  55%|██████████████▊            | 109/199 [01:02<00:46,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  55%|██████████████▉            | 110/199 [01:02<00:45,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  56%|███████████████            | 111/199 [01:03<00:45,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  56%|███████████████▏           | 112/199 [01:03<00:44,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  57%|███████████████▎           | 113/199 [01:04<00:43,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  57%|███████████████▍           | 114/199 [01:04<00:43,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  58%|███████████████▌           | 115/199 [01:05<00:43,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  58%|███████████████▋           | 116/199 [01:05<00:42,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  59%|███████████████▊           | 117/199 [01:06<00:42,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  59%|████████████████           | 118/199 [01:06<00:41,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  60%|████████████████▏          | 119/199 [01:07<00:40,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  60%|████████████████▎          | 120/199 [01:07<00:40,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  61%|████████████████▍          | 121/199 [01:08<00:39,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  61%|████████████████▌          | 122/199 [01:08<00:39,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  62%|████████████████▋          | 123/199 [01:09<00:38,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  62%|████████████████▊          | 124/199 [01:09<00:38,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  63%|████████████████▉          | 125/199 [01:10<00:37,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  63%|█████████████████          | 126/199 [01:10<00:37,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  64%|█████████████████▏         | 127/199 [01:11<00:36,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  64%|█████████████████▎         | 128/199 [01:11<00:36,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  65%|█████████████████▌         | 129/199 [01:12<00:35,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  65%|█████████████████▋         | 130/199 [01:12<00:35,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  66%|█████████████████▊         | 131/199 [01:13<00:34,  1.96it/s]\u001b[A\n",
      "DDIM Sampler:  66%|█████████████████▉         | 132/199 [01:13<00:34,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  67%|██████████████████         | 133/199 [01:14<00:33,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  67%|██████████████████▏        | 134/199 [01:14<00:33,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  68%|██████████████████▎        | 135/199 [01:15<00:32,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  68%|██████████████████▍        | 136/199 [01:15<00:32,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  69%|██████████████████▌        | 137/199 [01:16<00:31,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  69%|██████████████████▋        | 138/199 [01:17<00:31,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  70%|██████████████████▊        | 139/199 [01:17<00:30,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  72%|███████████████████▌       | 144/199 [01:18<00:27,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  73%|███████████████████▋       | 145/199 [01:19<00:27,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  73%|███████████████████▊       | 146/199 [01:19<00:26,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  74%|███████████████████▉       | 147/199 [01:20<00:26,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  74%|████████████████████       | 148/199 [01:20<00:25,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  75%|████████████████████▏      | 149/199 [01:21<00:25,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  75%|████████████████████▎      | 150/199 [01:21<00:24,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  76%|████████████████████▍      | 151/199 [01:22<00:24,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  76%|████████████████████▌      | 152/199 [01:22<00:23,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  77%|████████████████████▊      | 153/199 [01:23<00:23,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  77%|████████████████████▉      | 154/199 [01:23<00:22,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  78%|█████████████████████      | 155/199 [01:24<00:22,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  76%|████████████████████▌      | 152/199 [01:24<00:24,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  77%|████████████████████▊      | 153/199 [01:24<00:23,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  77%|████████████████████▉      | 154/199 [01:25<00:23,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  78%|█████████████████████      | 155/199 [01:25<00:22,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  78%|█████████████████████▏     | 156/199 [01:26<00:22,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  79%|█████████████████████▎     | 157/199 [01:26<00:21,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  79%|█████████████████████▍     | 158/199 [01:27<00:21,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  80%|█████████████████████▌     | 159/199 [01:27<00:20,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  80%|█████████████████████▋     | 160/199 [01:28<00:19,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  81%|█████████████████████▊     | 161/199 [01:28<00:19,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  81%|█████████████████████▉     | 162/199 [01:29<00:18,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  82%|██████████████████████     | 163/199 [01:29<00:18,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  82%|██████████████████████▎    | 164/199 [01:30<00:17,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  83%|██████████████████████▍    | 165/199 [01:30<00:17,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  83%|██████████████████████▌    | 166/199 [01:31<00:16,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  84%|██████████████████████▋    | 167/199 [01:31<00:16,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  84%|██████████████████████▊    | 168/199 [01:32<00:15,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  85%|██████████████████████▉    | 169/199 [01:32<00:15,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  85%|███████████████████████    | 170/199 [01:33<00:14,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  86%|███████████████████████▏   | 171/199 [01:33<00:14,  1.94it/s]\u001b[A\n",
      "DDIM Sampler:  86%|███████████████████████▎   | 172/199 [01:34<00:13,  1.94it/s]\u001b[A\n",
      "DDIM Sampler:  87%|███████████████████████▍   | 173/199 [01:34<00:13,  1.94it/s]\u001b[A\n",
      "DDIM Sampler:  87%|███████████████████████▌   | 174/199 [01:35<00:12,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  88%|███████████████████████▋   | 175/199 [01:35<00:12,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  88%|███████████████████████▉   | 176/199 [01:36<00:11,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  89%|████████████████████████   | 177/199 [01:36<00:11,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  89%|████████████████████████▏  | 178/199 [01:37<00:10,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  90%|████████████████████████▎  | 179/199 [01:37<00:10,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  90%|████████████████████████▍  | 180/199 [01:38<00:09,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  91%|████████████████████████▌  | 181/199 [01:38<00:09,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  91%|████████████████████████▋  | 182/199 [01:39<00:08,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  92%|████████████████████████▊  | 183/199 [01:40<00:08,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  92%|████████████████████████▉  | 184/199 [01:40<00:07,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  93%|█████████████████████████  | 185/199 [01:41<00:07,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  95%|█████████████████████████▊ | 190/199 [01:42<00:04,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  96%|█████████████████████████▉ | 191/199 [01:42<00:04,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  96%|██████████████████████████ | 192/199 [01:43<00:03,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  97%|██████████████████████████▏| 193/199 [01:43<00:03,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  97%|██████████████████████████▎| 194/199 [01:44<00:02,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  98%|██████████████████████████▍| 195/199 [01:44<00:02,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  98%|██████████████████████████▌| 196/199 [01:45<00:01,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  99%|██████████████████████████▋| 197/199 [01:45<00:01,  1.98it/s]\u001b[A\n",
      "DDIM Sampler:  99%|██████████████████████████▊| 198/199 [01:46<00:00,  1.98it/s]\u001b[A\n",
      "DDIM Sampler: 100%|███████████████████████████| 199/199 [01:46<00:00,  1.87it/s]\u001b[A\n",
      "Sampling with classifier-free guidance: Restored training weights\n",
      "\n",
      "DDIM Sampler:  98%|██████████████████████████▍| 195/199 [01:46<00:02,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  98%|██████████████████████████▌| 196/199 [01:46<00:01,  1.95it/s]\u001b[A\n",
      "DDIM Sampler:  99%|██████████████████████████▋| 197/199 [01:47<00:01,  1.95it/s]\u001b[A\n",
      "DDIM Sampler: 100%|███████████████████████████| 199/199 [01:47<00:00,  1.85it/s]\u001b[A\n",
      "Sampling with classifier-free guidance: Restored training weights\n",
      "DDIM Sampler: 100%|███████████████████████████| 199/199 [01:48<00:00,  1.84it/s]\n",
      "Sampling with classifier-free guidance: Restored training weights\n",
      "DDIM Sampler: 100%|███████████████████████████| 199/199 [01:48<00:00,  1.84it/s]\n",
      "Sampling with classifier-free guidance: Restored training weights\n",
      "/usr/lib/python3/dist-packages/torch/utils/tensorboard/summary.py:486: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS)\n",
      "/usr/lib/python3/dist-packages/torch/utils/tensorboard/summary.py:486: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS)\n",
      "/usr/lib/python3/dist-packages/torch/utils/tensorboard/summary.py:486: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS)\n",
      "/usr/lib/python3/dist-packages/torch/utils/tensorboard/summary.py:486: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS)\n",
      "/usr/lib/python3/dist-packages/torch/utils/tensorboard/summary.py:486: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  image = image.resize((scaled_width, scaled_height), Image.ANTIALIAS)\n",
      "Epoch 0: 100%|█| 15/15 [04:25<00:00, 16.62s/it, loss=0.0197, v_num=0, train/loss/home/ubuntu/.local/lib/python3.8/site-packages/pytorch_lightning/trainer/callback_hook.py:102: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5\n",
      "  warning_cache.deprecation(\n",
      "Average Epoch time: 266.04 seconds\n",
      "Average Peak memory 37524.60MiB\n",
      "Epoch 1: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.019, v_num=0, train/loss_Average Epoch time: 63.90 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 2: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0193, v_num=0, train/lossAverage Epoch time: 64.06 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 3: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0193, v_num=0, train/lossAverage Epoch time: 64.14 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 4: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0223, v_num=0, train/lossAverage Epoch time: 63.93 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 5: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0197, v_num=0, train/lossAverage Epoch time: 64.03 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 6: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0183, v_num=0, train/lossAverage Epoch time: 64.02 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 7: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0197, v_num=0, train/lossAverage Epoch time: 63.68 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 8: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0206, v_num=0, train/lossAverage Epoch time: 63.84 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 9:  94%|▉| 15/16 [01:03<00:03,  3.99s/it, loss=0.0204, v_num=0, train/loss\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|█| 16/16 [01:06<00:00,  3.92s/it, loss=0.0204, v_num=0, train/loss\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.62 seconds\n",
      "Average Peak memory 33276.53MiB\n",
      "Epoch 10: 100%|█| 15/15 [01:02<00:00,  3.92s/it, loss=0.0168, v_num=0, train/losAverage Epoch time: 62.80 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 11: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0164, v_num=0, train/losAverage Epoch time: 63.99 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 12: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0165, v_num=0, train/losAverage Epoch time: 63.78 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 13: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0185, v_num=0, train/losAverage Epoch time: 63.79 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 14: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0181, v_num=0, train/losAverage Epoch time: 63.87 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 14, global step 224: train/loss reached 0.01615 (best 0.01615), saving model to \"../checkpoints/epoch=14-step=224.ckpt\" as top 1\n",
      "Epoch 15: 100%|█| 15/15 [01:01<00:00,  3.87s/it, loss=0.0193, v_num=0, train/losAverage Epoch time: 61.92 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 16: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0159, v_num=0, train/losAverage Epoch time: 63.84 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 17: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0148, v_num=0, train/losAverage Epoch time: 63.74 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 18: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0142, v_num=0, train/losAverage Epoch time: 63.92 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 19:  94%|▉| 15/16 [01:03<00:03,  4.00s/it, loss=0.0157, v_num=0, train/los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|█| 16/16 [01:06<00:00,  3.92s/it, loss=0.0157, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.73 seconds\n",
      "Average Peak memory 33277.10MiB\n",
      "Epoch 20: 100%|█| 15/15 [01:02<00:00,  3.94s/it, loss=0.0154, v_num=0, train/losAverage Epoch time: 63.02 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 21: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0197, v_num=0, train/losAverage Epoch time: 63.85 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 22: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0157, v_num=0, train/losAverage Epoch time: 63.99 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 23: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0183, v_num=0, train/losAverage Epoch time: 64.16 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 24: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0162, v_num=0, train/losAverage Epoch time: 63.92 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 25: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0168, v_num=0, train/losAverage Epoch time: 64.15 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 26: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0157, v_num=0, train/losAverage Epoch time: 63.97 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 27: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.013, v_num=0, train/lossAverage Epoch time: 63.79 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 28: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.013, v_num=0, train/lossAverage Epoch time: 63.69 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 29:  94%|▉| 15/16 [01:03<00:03,  3.99s/it, loss=0.0181, v_num=0, train/los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 29: 100%|█| 16/16 [01:06<00:00,  3.91s/it, loss=0.0181, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.55 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 29, global step 449: train/loss reached 0.01726 (best 0.01615), saving model to \"../checkpoints/epoch=29-step=449.ckpt\" as top 2\n",
      "Epoch 30: 100%|█| 15/15 [01:00<00:00,  3.81s/it, loss=0.016, v_num=0, train/lossAverage Epoch time: 61.00 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 31: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.012, v_num=0, train/lossAverage Epoch time: 63.91 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 32: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0141, v_num=0, train/losAverage Epoch time: 64.04 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 33: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0156, v_num=0, train/losAverage Epoch time: 63.91 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 34: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0202, v_num=0, train/losAverage Epoch time: 63.93 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 35: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0176, v_num=0, train/losAverage Epoch time: 64.13 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 36: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0141, v_num=0, train/losAverage Epoch time: 64.04 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 37: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0144, v_num=0, train/losAverage Epoch time: 63.73 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 38: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0118, v_num=0, train/losAverage Epoch time: 63.90 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 39:  94%|▉| 15/16 [01:03<00:03,  3.99s/it, loss=0.0172, v_num=0, train/los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 39: 100%|█| 16/16 [01:06<00:00,  3.92s/it, loss=0.0172, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.61 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 40: 100%|█| 15/15 [01:02<00:00,  3.93s/it, loss=0.0194, v_num=0, train/losAverage Epoch time: 62.94 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 41: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0174, v_num=0, train/losAverage Epoch time: 64.05 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 42: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0168, v_num=0, train/losAverage Epoch time: 63.93 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 43: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.018, v_num=0, train/lossAverage Epoch time: 64.10 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 44: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0164, v_num=0, train/losAverage Epoch time: 64.10 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 44, global step 674: train/loss reached 0.01505 (best 0.01505), saving model to \"../checkpoints/epoch=44-step=674.ckpt\" as top 3\n",
      "Epoch 45: 100%|█| 15/15 [01:01<00:00,  3.87s/it, loss=0.0176, v_num=0, train/losAverage Epoch time: 61.95 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 46: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0163, v_num=0, train/losAverage Epoch time: 64.07 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 47: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0157, v_num=0, train/losAverage Epoch time: 63.75 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 48: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0139, v_num=0, train/losAverage Epoch time: 64.12 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 49:  94%|▉| 15/16 [01:03<00:03,  3.99s/it, loss=0.0115, v_num=0, train/los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 49: 100%|█| 16/16 [01:06<00:00,  3.93s/it, loss=0.0115, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.76 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 50: 100%|█| 15/15 [01:02<00:00,  3.92s/it, loss=0.0156, v_num=0, train/losAverage Epoch time: 62.83 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 51: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0168, v_num=0, train/losAverage Epoch time: 63.66 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 52: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0116, v_num=0, train/losAverage Epoch time: 63.77 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 53: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0122, v_num=0, train/losAverage Epoch time: 64.19 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 54: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0137, v_num=0, train/losAverage Epoch time: 63.91 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 55: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0128, v_num=0, train/losAverage Epoch time: 63.94 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 56: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.011, v_num=0, train/lossAverage Epoch time: 64.15 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 57: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0111, v_num=0, train/losAverage Epoch time: 63.94 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 58: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0129, v_num=0, train/losAverage Epoch time: 63.93 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 59:  94%|▉| 15/16 [01:03<00:03,  3.99s/it, loss=0.0121, v_num=0, train/los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 59: 100%|█| 16/16 [01:06<00:00,  3.93s/it, loss=0.0121, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.76 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 59, global step 899: train/loss reached 0.01323 (best 0.01323), saving model to \"../checkpoints/epoch=59-step=899.ckpt\" as top 4\n",
      "Epoch 60: 100%|█| 15/15 [01:01<00:00,  3.82s/it, loss=0.0119, v_num=0, train/losAverage Epoch time: 61.09 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 61: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0122, v_num=0, train/losAverage Epoch time: 63.96 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 62: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0122, v_num=0, train/losAverage Epoch time: 64.02 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 63: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0113, v_num=0, train/losAverage Epoch time: 64.23 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 64: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0107, v_num=0, train/losAverage Epoch time: 64.17 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 65: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0113, v_num=0, train/losAverage Epoch time: 63.95 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 66: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0148, v_num=0, train/losAverage Epoch time: 63.89 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 67: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0119, v_num=0, train/losAverage Epoch time: 64.13 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 68: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0167, v_num=0, train/losAverage Epoch time: 64.07 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 69:  94%|▉| 15/16 [01:04<00:04,  4.01s/it, loss=0.015, v_num=0, train/loss\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 69: 100%|█| 16/16 [01:06<00:00,  3.94s/it, loss=0.015, v_num=0, train/loss\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.96 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 70: 100%|█| 15/15 [01:02<00:00,  3.93s/it, loss=0.0124, v_num=0, train/losAverage Epoch time: 62.97 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 71: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0147, v_num=0, train/losAverage Epoch time: 64.04 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 72: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0147, v_num=0, train/losAverage Epoch time: 63.81 seconds\n",
      "Average Peak memory 33277.45MiB\n",
      "Epoch 73: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0126, v_num=0, train/losAverage Epoch time: 64.19 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 74: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0115, v_num=0, train/losAverage Epoch time: 63.95 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 74, global step 1124: train/loss reached 0.01341 (best 0.01323), saving model to \"../checkpoints/epoch=74-step=1124.ckpt\" as top 5\n",
      "Epoch 75: 100%|█| 15/15 [01:02<00:00,  3.88s/it, loss=0.0122, v_num=0, train/losAverage Epoch time: 62.12 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 76: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0105, v_num=0, train/losAverage Epoch time: 64.12 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 77: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.015, v_num=0, train/lossAverage Epoch time: 64.04 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 78: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0112, v_num=0, train/losAverage Epoch time: 63.81 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 79:  94%|▉| 15/16 [01:04<00:04,  4.01s/it, loss=0.0101, v_num=0, train/los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 79: 100%|█| 16/16 [01:06<00:00,  3.93s/it, loss=0.0101, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.81 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 80: 100%|█| 15/15 [01:02<00:00,  3.94s/it, loss=0.00962, v_num=0, train/loAverage Epoch time: 63.04 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 81: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0109, v_num=0, train/losAverage Epoch time: 64.20 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 82: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.0122, v_num=0, train/losAverage Epoch time: 63.77 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 83: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0113, v_num=0, train/losAverage Epoch time: 63.99 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 84: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0138, v_num=0, train/losAverage Epoch time: 63.87 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 85: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0111, v_num=0, train/losAverage Epoch time: 64.02 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 86: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0135, v_num=0, train/losAverage Epoch time: 64.15 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 87: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0111, v_num=0, train/losAverage Epoch time: 63.84 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 88: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0103, v_num=0, train/losAverage Epoch time: 64.01 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 89:  94%|▉| 15/16 [01:03<00:03,  3.98s/it, loss=0.0121, v_num=0, train/los\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 89: 100%|█| 16/16 [01:06<00:00,  3.91s/it, loss=0.0121, v_num=0, train/los\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.47 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 89, global step 1349: train/loss reached 0.01408 (best 0.01323), saving model to \"../checkpoints/epoch=89-step=1349.ckpt\" as top 6\n",
      "Epoch 90: 100%|█| 15/15 [01:01<00:00,  3.82s/it, loss=0.01, v_num=0, train/loss_Average Epoch time: 61.23 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 91: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0101, v_num=0, train/losAverage Epoch time: 63.86 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 92: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.012, v_num=0, train/lossAverage Epoch time: 63.89 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 93: 100%|█| 15/15 [01:03<00:00,  4.00s/it, loss=0.0098, v_num=0, train/losAverage Epoch time: 64.03 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 94: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.00946, v_num=0, train/loAverage Epoch time: 64.06 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 95: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0064, v_num=0, train/losAverage Epoch time: 63.92 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 96: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.00841, v_num=0, train/loAverage Epoch time: 64.13 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 97: 100%|█| 15/15 [01:03<00:00,  3.99s/it, loss=0.0121, v_num=0, train/losAverage Epoch time: 63.96 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 98: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0129, v_num=0, train/losAverage Epoch time: 64.10 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 99:  94%|▉| 15/16 [01:03<00:03,  3.99s/it, loss=0.00956, v_num=0, train/lo\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating:   0%|                                         | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 99: 100%|█| 16/16 [01:06<00:00,  3.92s/it, loss=0.00956, v_num=0, train/lo\u001b[A\n",
      "                                                                                \u001b[AAverage Epoch time: 66.63 seconds\n",
      "Average Peak memory 33277.44MiB\n",
      "Epoch 100: 100%|█| 15/15 [01:02<00:00,  3.93s/it, loss=0.00961, v_num=0, train/lAverage Epoch time: 62.89 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 101: 100%|█| 15/15 [01:03<00:00,  3.98s/it, loss=0.00761, v_num=0, train/lAverage Epoch time: 63.80 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 102: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.0122, v_num=0, train/loAverage Epoch time: 64.08 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 103: 100%|█| 15/15 [01:04<00:00,  4.01s/it, loss=0.0141, v_num=0, train/loAverage Epoch time: 64.17 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 104: 100%|█| 15/15 [01:04<00:00,  4.00s/it, loss=0.00925, v_num=0, train/lAverage Epoch time: 64.07 seconds\n",
      "Average Peak memory 33277.41MiB\n",
      "Epoch 104, global step 1574: train/loss reached 0.00997 (best 0.00997), saving model to \"../checkpoints/epoch=104-step=1574.ckpt\" as top 7\n",
      "Epoch 105:  60%|▌| 9/15 [00:38<00:23,  3.90s/it, loss=0.0109, v_num=0, train/los^C\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 1xA100\n",
    "# Run training\n",
    "!(python main.py \\\n",
    "    -t \\\n",
    "    --base ../skribbl.yaml \\\n",
    "    --gpus=\"1\" \\\n",
    "    --scale_lr False \\\n",
    "    --num_nodes 1 \\\n",
    "    --check_val_every_n_epoch 10 \\\n",
    "    --finetune_from \"$ckpt_path\" \\\n",
    "    data.params.batch_size=\"$BATCH_SIZE\" \\\n",
    "    lightning.trainer.accumulate_grad_batches=\"$ACCUMULATE_BATCHES\" \\\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# 4xA6000\n",
    "# Run training\n",
    "!(python main.py \\\n",
    "    -t \\\n",
    "    --base ../skribbl.yaml \\\n",
    "    --gpus \"$gpu_list\" \\\n",
    "    --scale_lr False \\\n",
    "    --num_nodes 1 \\\n",
    "    --check_val_every_n_epoch 10 \\\n",
    "    --finetune_from \"$ckpt_path\" \\\n",
    "    data.params.batch_size=\"$BATCH_SIZE\" \\\n",
    "    lightning.trainer.accumulate_grad_batches=\"$ACCUMULATE_BATCHES\" \\\n",
    "    data.params.validation.params.n_gpus=\"$N_GPUS\" \\\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=1-step=29.ckpt'      'epoch=44-step=674.ckpt'\n",
      "'epoch=1.ckpt'              'epoch=59-step=899.ckpt'\n",
      "'epoch=104-step=1574.ckpt'  'epoch=74-step=1124.ckpt'\n",
      "'epoch=14-step=224.ckpt'    'epoch=89-step=1349.ckpt'\n",
      "'epoch=29-step=449.ckpt'     last.ckpt\n"
     ]
    }
   ],
   "source": [
    "%ls ../checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "sk9wHFKgz8kJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "Global seed set to 42\n",
      "Loading model from ../checkpoints/epoch=104-step=1574.ckpt\n",
      "Global Step: 1575\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "DiffusionWrapper has 859.52 M params.\n",
      "Keeping EMAs of 688.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Some weights of the model checkpoint at openai/clip-vit-large-patch14 were not used when initializing CLIPTextModel: ['vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.mlp.fc2.bias', 'vision_model.encoder.layers.21.mlp.fc1.bias', 'vision_model.encoder.layers.23.layer_norm2.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.bias', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.bias', 'vision_model.encoder.layers.15.mlp.fc1.bias', 'vision_model.encoder.layers.2.layer_norm1.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.bias', 'vision_model.encoder.layers.9.layer_norm1.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.bias', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.layer_norm1.weight', 'vision_model.pre_layrnorm.bias', 'vision_model.encoder.layers.23.self_attn.k_proj.bias', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.bias', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.bias', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.pre_layrnorm.weight', 'vision_model.encoder.layers.14.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.layer_norm2.bias', 'vision_model.encoder.layers.14.layer_norm2.weight', 'vision_model.encoder.layers.15.layer_norm1.bias', 'vision_model.encoder.layers.5.mlp.fc1.bias', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.6.layer_norm2.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.16.layer_norm2.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.embeddings.position_ids', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm2.bias', 'vision_model.encoder.layers.17.self_attn.out_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.weight', 'vision_model.encoder.layers.10.mlp.fc1.bias', 'vision_model.encoder.layers.10.layer_norm1.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.15.layer_norm2.weight', 'vision_model.encoder.layers.21.layer_norm2.bias', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.embeddings.position_embedding.weight', 'vision_model.encoder.layers.7.layer_norm2.bias', 'vision_model.encoder.layers.21.layer_norm1.bias', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.bias', 'vision_model.encoder.layers.19.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.mlp.fc2.bias', 'vision_model.encoder.layers.12.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.bias', 'vision_model.encoder.layers.6.mlp.fc2.bias', 'vision_model.encoder.layers.15.mlp.fc2.bias', 'vision_model.encoder.layers.13.layer_norm2.weight', 'vision_model.encoder.layers.7.layer_norm1.bias', 'vision_model.encoder.layers.8.mlp.fc1.bias', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.layer_norm1.bias', 'vision_model.encoder.layers.9.layer_norm2.bias', 'vision_model.encoder.layers.4.layer_norm2.bias', 'vision_model.encoder.layers.16.layer_norm1.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.bias', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.bias', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.bias', 'vision_model.encoder.layers.2.mlp.fc1.bias', 'vision_model.encoder.layers.16.mlp.fc1.bias', 'vision_model.encoder.layers.18.mlp.fc1.bias', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.bias', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.bias', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.bias', 'vision_model.encoder.layers.18.layer_norm1.weight', 'vision_model.encoder.layers.8.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.bias', 'vision_model.encoder.layers.4.layer_norm1.bias', 'vision_model.encoder.layers.11.layer_norm1.bias', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.bias', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.layer_norm1.bias', 'vision_model.encoder.layers.19.self_attn.k_proj.bias', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.10.layer_norm2.bias', 'vision_model.encoder.layers.12.layer_norm2.bias', 'vision_model.encoder.layers.19.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm1.weight', 'vision_model.encoder.layers.3.layer_norm1.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.bias', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.layer_norm2.weight', 'vision_model.encoder.layers.6.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm1.bias', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.embeddings.patch_embedding.weight', 'vision_model.encoder.layers.22.layer_norm1.weight', 'vision_model.encoder.layers.17.layer_norm1.weight', 'vision_model.encoder.layers.15.layer_norm2.bias', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.2.layer_norm1.bias', 'vision_model.encoder.layers.14.mlp.fc2.bias', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.layer_norm2.bias', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.bias', 'vision_model.encoder.layers.16.layer_norm2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.bias', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.3.mlp.fc1.bias', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.bias', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.22.layer_norm2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.mlp.fc2.bias', 'vision_model.encoder.layers.22.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.layer_norm2.bias', 'vision_model.encoder.layers.2.self_attn.v_proj.bias', 'visual_projection.weight', 'vision_model.encoder.layers.10.layer_norm1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.23.layer_norm1.bias', 'vision_model.encoder.layers.1.layer_norm1.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.mlp.fc2.bias', 'vision_model.encoder.layers.9.mlp.fc1.bias', 'vision_model.encoder.layers.0.layer_norm2.bias', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.17.mlp.fc1.bias', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.layer_norm1.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.bias', 'vision_model.encoder.layers.9.self_attn.v_proj.bias', 'vision_model.encoder.layers.13.mlp.fc1.bias', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.bias', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.bias', 'vision_model.encoder.layers.23.mlp.fc1.bias', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.12.layer_norm2.weight', 'vision_model.encoder.layers.23.layer_norm1.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.bias', 'vision_model.encoder.layers.6.layer_norm1.bias', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.bias', 'logit_scale', 'vision_model.encoder.layers.4.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.layer_norm2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.layer_norm2.weight', 'vision_model.encoder.layers.6.layer_norm1.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.bias', 'vision_model.encoder.layers.7.mlp.fc2.bias', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.bias', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.layer_norm2.bias', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.bias', 'vision_model.encoder.layers.10.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.7.layer_norm1.weight', 'vision_model.encoder.layers.7.mlp.fc1.bias', 'vision_model.encoder.layers.4.mlp.fc2.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.bias', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.layer_norm1.bias', 'vision_model.encoder.layers.3.self_attn.q_proj.bias', 'vision_model.encoder.layers.3.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.layer_norm1.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.bias', 'vision_model.encoder.layers.20.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.layer_norm1.bias', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc2.bias', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.bias', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.bias', 'vision_model.encoder.layers.19.layer_norm2.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.bias', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.layer_norm1.weight', 'vision_model.encoder.layers.5.layer_norm1.bias', 'vision_model.encoder.layers.16.mlp.fc2.bias', 'vision_model.encoder.layers.15.self_attn.k_proj.bias', 'vision_model.encoder.layers.9.self_attn.out_proj.bias', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.bias', 'vision_model.encoder.layers.21.self_attn.k_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.bias', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.bias', 'vision_model.encoder.layers.9.self_attn.k_proj.bias', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.layer_norm1.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.layer_norm2.bias', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.bias', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.v_proj.bias', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.bias', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.layer_norm2.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.bias', 'vision_model.encoder.layers.12.layer_norm1.weight', 'vision_model.encoder.layers.1.layer_norm2.bias', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.bias', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.bias', 'vision_model.encoder.layers.14.layer_norm1.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.bias', 'vision_model.encoder.layers.18.mlp.fc2.bias', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc1.bias', 'vision_model.encoder.layers.6.self_attn.k_proj.bias', 'vision_model.encoder.layers.7.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.layer_norm1.bias', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.bias', 'vision_model.encoder.layers.13.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.bias', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.layer_norm2.weight', 'vision_model.encoder.layers.4.layer_norm1.weight', 'vision_model.embeddings.class_embedding', 'vision_model.encoder.layers.12.mlp.fc2.bias', 'vision_model.encoder.layers.22.mlp.fc1.bias', 'vision_model.encoder.layers.17.layer_norm2.weight', 'vision_model.encoder.layers.13.layer_norm1.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.layer_norm1.bias', 'vision_model.encoder.layers.10.self_attn.k_proj.bias', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.20.layer_norm2.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.layer_norm2.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.bias', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc2.bias', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.bias', 'text_projection.weight', 'vision_model.encoder.layers.13.layer_norm1.bias', 'vision_model.encoder.layers.10.layer_norm2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.bias', 'vision_model.encoder.layers.15.self_attn.v_proj.bias', 'vision_model.encoder.layers.11.self_attn.v_proj.bias', 'vision_model.encoder.layers.20.self_attn.out_proj.bias', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.5.layer_norm1.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.bias', 'vision_model.encoder.layers.11.self_attn.q_proj.bias', 'vision_model.encoder.layers.0.self_attn.q_proj.bias', 'vision_model.post_layernorm.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.mlp.fc2.bias', 'vision_model.encoder.layers.20.layer_norm2.bias', 'vision_model.encoder.layers.21.self_attn.v_proj.bias', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.bias', 'vision_model.encoder.layers.0.mlp.fc1.bias', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.layer_norm2.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.mlp.fc2.bias', 'vision_model.encoder.layers.7.self_attn.k_proj.bias', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.bias', 'vision_model.encoder.layers.8.self_attn.out_proj.bias', 'vision_model.encoder.layers.20.self_attn.q_proj.bias', 'vision_model.encoder.layers.21.layer_norm2.weight', 'vision_model.encoder.layers.11.mlp.fc2.bias', 'vision_model.encoder.layers.19.self_attn.v_proj.bias', 'vision_model.encoder.layers.18.layer_norm2.bias', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.bias', 'vision_model.post_layernorm.bias', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.bias', 'vision_model.encoder.layers.12.self_attn.out_proj.bias', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.layer_norm2.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.bias', 'vision_model.encoder.layers.22.self_attn.k_proj.bias', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.16.layer_norm1.weight', 'vision_model.encoder.layers.2.layer_norm2.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight']\n",
      "- This IS expected if you are initializing CLIPTextModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CLIPTextModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Sampling:   0%|                                           | 0/1 [00:00<?, ?it/s]\n",
      "data:   0%|                                               | 0/1 [00:00<?, ?it/s]\u001b[AData shape for DDIM sampling is (4, 4, 64, 64), eta 0.0\n",
      "Running DDIM Sampling with 49 timesteps\n",
      "\n",
      "\n",
      "DDIM Sampler:   0%|                                      | 0/49 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   2%|▌                             | 1/49 [00:01<01:18,  1.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   4%|█▏                            | 2/49 [00:02<00:45,  1.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   6%|█▊                            | 3/49 [00:02<00:34,  1.33it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:   8%|██▍                           | 4/49 [00:03<00:29,  1.54it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  10%|███                           | 5/49 [00:03<00:26,  1.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  12%|███▋                          | 6/49 [00:04<00:24,  1.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  14%|████▎                         | 7/49 [00:04<00:22,  1.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  16%|████▉                         | 8/49 [00:05<00:21,  1.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  18%|█████▌                        | 9/49 [00:05<00:20,  1.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  20%|█████▉                       | 10/49 [00:06<00:19,  1.97it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  22%|██████▌                      | 11/49 [00:06<00:19,  1.99it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  24%|███████                      | 12/49 [00:07<00:18,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  27%|███████▋                     | 13/49 [00:07<00:17,  2.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  29%|████████▎                    | 14/49 [00:08<00:17,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  31%|████████▉                    | 15/49 [00:08<00:16,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  33%|█████████▍                   | 16/49 [00:09<00:16,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  35%|██████████                   | 17/49 [00:09<00:15,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  37%|██████████▋                  | 18/49 [00:10<00:15,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  39%|███████████▏                 | 19/49 [00:10<00:14,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  41%|███████████▊                 | 20/49 [00:11<00:14,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  43%|████████████▍                | 21/49 [00:11<00:13,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  45%|█████████████                | 22/49 [00:12<00:13,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  47%|█████████████▌               | 23/49 [00:12<00:12,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  49%|██████████████▏              | 24/49 [00:13<00:12,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  51%|██████████████▊              | 25/49 [00:13<00:11,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  53%|███████████████▍             | 26/49 [00:14<00:11,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  55%|███████████████▉             | 27/49 [00:14<00:10,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  57%|████████████████▌            | 28/49 [00:15<00:10,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  59%|█████████████████▏           | 29/49 [00:15<00:09,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  61%|█████████████████▊           | 30/49 [00:15<00:09,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  63%|██████████████████▎          | 31/49 [00:16<00:08,  2.02it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  65%|██████████████████▉          | 32/49 [00:16<00:08,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  67%|███████████████████▌         | 33/49 [00:17<00:07,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  69%|████████████████████         | 34/49 [00:17<00:07,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  71%|████████████████████▋        | 35/49 [00:18<00:06,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  73%|█████████████████████▎       | 36/49 [00:18<00:06,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  76%|█████████████████████▉       | 37/49 [00:19<00:05,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  78%|██████████████████████▍      | 38/49 [00:19<00:05,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  80%|███████████████████████      | 39/49 [00:20<00:04,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  82%|███████████████████████▋     | 40/49 [00:20<00:04,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  84%|████████████████████████▎    | 41/49 [00:21<00:03,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  86%|████████████████████████▊    | 42/49 [00:21<00:03,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  88%|█████████████████████████▍   | 43/49 [00:22<00:02,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  90%|██████████████████████████   | 44/49 [00:22<00:02,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  92%|██████████████████████████▋  | 45/49 [00:23<00:01,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  94%|███████████████████████████▏ | 46/49 [00:23<00:01,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  96%|███████████████████████████▊ | 47/49 [00:24<00:00,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler:  98%|████████████████████████████▍| 48/49 [00:24<00:00,  2.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "DDIM Sampler: 100%|█████████████████████████████| 49/49 [00:25<00:00,  1.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "data: 100%|███████████████████████████████████████| 1/1 [00:26<00:00, 26.84s/it]\u001b[A\n",
      "Sampling: 100%|███████████████████████████████████| 1/1 [00:26<00:00, 26.84s/it]\n",
      "Your samples are ready and waiting for you here: \n",
      "../outputs/generated_drawings \n",
      "Sampling took 27.1845543384552s, i.e. produced 0.15 samples/sec. \n",
      "Enjoy.\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "!(python scripts/txt2img.py \\\n",
    "    --prompt 'simple microsoft paint drawing of a smile on a solid white background' \\\n",
    "    --outdir '../outputs/generated_drawings' \\\n",
    "    --H 512 --W 512 \\\n",
    "    --n_samples 4 \\\n",
    "    --config '../skribbl.yaml' \\\n",
    "    --ckpt \"../checkpoints/epoch=104-step=1574.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/checkpoints\n"
     ]
    }
   ],
   "source": [
    "%cd ../checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'epoch=1-step=29.ckpt'  'epoch=1.ckpt'   last.ckpt\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/stable-diffusion\n"
     ]
    }
   ],
   "source": [
    "%cd ../stable-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
